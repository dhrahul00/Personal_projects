{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e453ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import Network,FC_layer,AC_layer,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6dd1307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0th Epoch error : 105.36031865228671\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m network\u001b[38;5;241m.\u001b[39madd(AC_layer(Activation()\u001b[38;5;241m.\u001b[39msoftmax,Activation()\u001b[38;5;241m.\u001b[39msoftmax_dr))\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train the network\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mADAM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCross_entropy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     36\u001b[0m predictions \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\python\\Machine learning\\Deep Learning\\neural_network.py:238\u001b[0m, in \u001b[0;36mNetwork.fit\u001b[1;34m(self, X, y, epoch, learning_rate, batch_size, optimizer, loss_type, regularization)\u001b[0m\n\u001b[0;32m    236\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror(output,y[str_idx:stp_idx],batch_size,loss_type)\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m--> 238\u001b[0m         error \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mregu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m original_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X) \u001b[38;5;241m-\u001b[39m y))\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_list\u001b[38;5;241m.\u001b[39mappend(original_error)\n",
      "File \u001b[1;32m~\\python\\Machine learning\\Deep Learning\\neural_network.py:50\u001b[0m, in \u001b[0;36mFC_layer.backward_prop\u001b[1;34m(self, error, learning_rate, optimizer, t, regu)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_prop\u001b[39m(\u001b[38;5;28mself\u001b[39m,error,learning_rate,optimizer,t,regu): \n\u001b[0;32m     49\u001b[0m     lamda,b \u001b[38;5;241m=\u001b[39m regu\n\u001b[1;32m---> 50\u001b[0m     dw \u001b[38;5;241m=\u001b[39m \u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;129;43m@self\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlamda\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights_\u001b[49m\n\u001b[0;32m     51\u001b[0m     db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(error,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAG\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape the input data\n",
    "X_train = X_train.reshape(-1, 28 * 28)  # Flatten images to 1D (784 features)\n",
    "X_test = X_test.reshape(-1, 28 * 28)\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot encode the target labels\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Create a neural network instance\n",
    "network = Network()\n",
    "\n",
    "# Define the architecture of the network\n",
    "network.add(FC_layer(784,256))\n",
    "network.add(AC_layer(Activation().sigmoid,Activation().sigmoid_dr))\n",
    "network.add(FC_layer(256,128))\n",
    "network.add(AC_layer(Activation().tanh,Activation().tanh_prime))\n",
    "network.add(FC_layer(128,10))\n",
    "network.add(AC_layer(Activation().softmax,Activation().softmax_dr))\n",
    "\n",
    "# Train the network\n",
    "network.fit(X_train, y_train, epoch=20, learning_rate=0.01, batch_size=32, optimizer=\"ADAM\", loss_type=\"Cross_entropy\", regularization=None)\n",
    "\n",
    "# Make predictions\n",
    "predictions = network.predict(X_test)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert the one-hot encoded labels to class labels\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (y_pred == y_test_labels).mean()\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd693230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_deep",
   "language": "python",
   "name": "tensor_deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
